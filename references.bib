@inproceedings{tian2023justask,
  title     = {Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback},
  author    = {Tian, Katherine and Mitchell, Eric and Zhou, Allan and Sharma, Archit and Rafailov, Rafael and Yao, Huaxiu and Finn, Chelsea and Manning, Christopher D.},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year      = {2023},
  url       = {https://arxiv.org/abs/2305.14975}
}

@inproceedings{xiong2024uncertainty,
  title     = {Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs},
  author    = {Xiong, Miao and Hu, Zhiyuan and Lu, Xinyang and Li, Yifei and Fu, Jie and He, Junxian and Hooi, Bryan},
  booktitle = {Proceedings of the 2024 International Conference on Learning Representations (ICLR)},
  year      = {2024},
  url       = {https://arxiv.org/abs/2306.13063}
}

@article{kadavath2022know,
  title   = {Language Models (Mostly) Know What They Know},
  author  = {Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and others},
  journal = {arXiv preprint arXiv:2207.05221},
  year    = {2022},
  url     = {https://arxiv.org/abs/2207.05221}
}

@article{song2025introspect,
  title   = {Language Models Fail to Introspect About Their Knowledge of Language},
  author  = {Song, Siyuan and Hu, Jennifer and Mahowald, Kyle},
  journal = {arXiv preprint arXiv:2503.07513},
  year    = {2025},
  url     = {https://arxiv.org/abs/2503.07513}
}

@article{xu2023earthflat,
  title   = {The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation},
  author  = {Xu, Rongwu and Lin, Brian S. and Qiu, Han and others},
  journal = {arXiv preprint arXiv:2312.06717},
  year    = {2023},
  url     = {https://arxiv.org/abs/2312.06717}
}

@article{irving2018debate,
  title   = {AI Safety via Debate},
  author  = {Irving, Geoffrey and Christiano, Paul and Amodei, Dario},
  journal = {arXiv preprint arXiv:1805.00899},
  year    = {2018},
  url     = {https://arxiv.org/abs/1805.00899}
}

@article{browncohen2023debate,
  title   = {Scalable AI Safety via Doubly-Efficient Debate},
  author  = {Brown-Cohen, Jonah and Irving, Geoffrey and Piliouras, Georgios},
  journal = {arXiv preprint arXiv:2311.14125},
  year    = {2023},
  url     = {https://arxiv.org/abs/2311.14125}
}

@inproceedings{zhou2023epistemic,
  title     = {Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models},
  author    = {Zhou, Kaitlyn and Jurafsky, Dan and Hashimoto, Tatsunori},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year      = {2023},
  url       = {https://arxiv.org/abs/2302.13439}
}

@inproceedings{rivera2023assertive,
  title     = {Linguistic Assertiveness Affects Factuality Ratings and Model Behavior in QA Systems},
  author    = {Rivera, Colin and Ye, Xinyi and Kim, Yonsei and Li, Wenpeng},
  booktitle = {Findings of the Association for Computational Linguistics (ACL)},
  year      = {2023},
  url       = {https://arxiv.org/abs/2305.04745}
}

@misc{handa2025economictasksperformedai,
      title={Which Economic Tasks are Performed with AI? Evidence from Millions of Claude Conversations},
      author={Kunal Handa and Alex Tamkin and Miles McCain and Saffron Huang and Esin Durmus and Sarah Heck and Jared Mueller and Jerry Hong and Stuart Ritchie and Tim Belonax and Kevin K. Troy and Dario Amodei and Jared Kaplan and Jack Clark and Deep Ganguli},
      year={2025},
      eprint={2503.04761},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2503.04761},
}

@misc{zheng2025deepresearcherscalingdeepresearch,
      title={DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments},
      author={Yuxiang Zheng and Dayuan Fu and Xiangkun Hu and Xiaojie Cai and Lyumanshan Ye and Pengrui Lu and Pengfei Liu},
      year={2025},
      eprint={2504.03160},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2504.03160},
}

@article{Li2024ConfidenceMR,
  title={Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models},
  author={Loka Li and Guan-Hong Chen and Yusheng Su and Zhenhao Chen and Yixuan Zhang and Eric P. Xing and Kun Zhang},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.12563},
  url={https://api.semanticscholar.org/CorpusID:268032763}
}

@misc{west2025basemodelsbeataligned,
      title={Base Models Beat Aligned Models at Randomness and Creativity},
      author={Peter West and Christopher Potts},
      year={2025},
      eprint={2505.00047},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.00047},
}

@article{Moore2008,
  author = {Moore, Don A. and Healy, Paul J.},
  title = {The Trouble With Overconfidence},
  journal = {Psychological Review},
  year = {2008},
  volume = {115},
  number = {2},
  pages = {502--517},
  doi = {https://doi.org/10.1037/0033-295X.115.2.502},
}

@article{GriffinTversky1992,
  author = {Griffin, Dale and Tversky, Amos},
  title = {The Weighing of Evidence and the Determinants of Confidence},
  journal = {Cognitive Psychology},
  year = {1992},
  volume = {24},
  number = {3},
  pages = {411--435},
  doi = {https://doi.org/10.1016/0010-0285(92)90013-R}
}

@article{benjamin2018errors,
  title={Errors in Probabilistic Reasoning and Judgment Biases},
  author={Benjamin, Daniel J. and Benjamin, Daniel J.},
  journal={USC-INET Research Paper},
  number={18-21},
  year={2018},
  month={October},
  day={15},
  url={https://ssrn.com/abstract=3293360},
  doi={10.2139/ssrn.3293360}
}


@inbook{Lichtenstein1977,
	abstract = {From the subjectivist point of view (de Finetti, 1937) a probability is a degree of belief in a proposition whose truth has not been ascertained. A probability expresses a purely internal state; there is no ``right'' or ``correct'' probability that resides somewhere ``in reality'' against which it can be compared. However, in many circumstances, it may become possible to verify the truth o{\textsterling} falsity of the proposition to which a probability was attached. Today, we assess the probability of the proposition``it will rain tomorrow''. Tomorrow, we go outside and look at the rain gauge to see whether or not it has rained. When verification is possible, we can use it to gauge the adequacy of our probability assessments.},
	address = {Dordrecht},
	author = {Lichtenstein, Sarah and Fischhoff, Baruch and Phillips, Lawrence D.},
	booktitle = {Decision Making and Change in Human Affairs: Proceedings of the Fifth Research Conference on Subjective Probability, Utility, and Decision Making, Darmstadt, 1--4 September, 1975},
	doi = {10.1007/978-94-010-1276-8_19},
	editor = {Jungermann, Helmut and De Zeeuw, Gerard},
	isbn = {978-94-010-1276-8},
	pages = {275--324},
	publisher = {Springer Netherlands},
	title = {Calibration of Probabilities: The State of the Art},
	url = {https://doi.org/10.1007/978-94-010-1276-8_19},
	year = {1977},
	bdsk-url-1 = {https://doi.org/10.1007/978-94-010-1276-8_19}}

@article{Hashim2024,
  author = {Hashim, Muhammad J.},
  title = {Verbal Probability Terms for Communicating Clinical Risk - a Systematic Review},
  journal = {Ulster Medical Journal},
  volume = {93},
  number = {1},
  pages = {18--23},
  year = {2024},
  month = {Jan},
  note = {Epub 2024 May 3},
  pmid = {38707974},
  pmcid = {PMC11067312}
}

@incollection{Mandel2019,
  author = {Mandel, David R.},
  title = {Systematic Monitoring of Forecasting Skill in Strategic Intelligence},
  booktitle = {Assessment and Communication of Uncertainty in Intelligence to Support Decision Making: Final Report of Research Task Group SAS-114},
  editor = {Mandel, David R.},
  publisher = {NATO Science and Technology Organization},
  address = {Brussels, Belgium},
  year = {2019},
  month = {March},
  note = {Posted: 15 Aug 2019, Conditionally accepted},
  pages = {16},
  url = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3435945}
}

@TechReport{RePEc:sip:dpaper:06-042,
  author={Jonathan Meer and Edward Van Wesep},
  title={{A Test of Confidence Enhanced Performance: Evidence from US College Debaters}},
  year=2007,
  month=Aug,
  institution={Stanford Institute for Economic Policy Research},
  type={Discussion Papers},
  url={https://ideas.repec.org/p/sip/dpaper/06-042.html},
  number={06-042},
  abstract={We test the theory put forth by Compte and Postlewaite (2004) that overconfidence might persist because it is welfare improving. They argue that because confidence enhances performance, some overconfidence is optimal in spite of its negative effect on decision-making. One implication of their model is that while an agent’s bias (first moment of prediction error) may not change as she gains experience in an activity, her predictive accuracy (second moment of prediction error) should improve. We test this implication by comparing predictions of success by university debaters with outcomes in debate rounds and evaluating how the first and second moments of their prediction errors change with experience. As predicted by the theory, we find that while debaters remain overconfident in spite of experience, they become more accurate in their predictions. These findings support the view that overconfidence may persist because it is welfare improving.},
  keywords={overconfidence},
  doi={},
}

@misc{chhikara2025mindconfidencegapoverconfidence,
      title={Mind the Confidence Gap: Overconfidence, Calibration, and Distractor Effects in Large Language Models}, 
      author={Prateek Chhikara},
      year={2025},
      eprint={2502.11028},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.11028}, 
}

@inproceedings{wen2024from,
title={From Human to Model Overconfidence: Evaluating Confidence Dynamics in Large Language Models},
author={Bingbing Wen and Chenjun Xu and Bin HAN and Robert Wolfe and Lucy Lu Wang and Bill Howe},
booktitle={NeurIPS 2024 Workshop on Behavioral Machine Learning},
year={2024},
url={https://openreview.net/forum?id=y9UdO5cmHs}
}

@misc{leng2025tamingoverconfidencellmsreward,
      title={Taming Overconfidence in LLMs: Reward Calibration in RLHF}, 
      author={Jixuan Leng and Chengsong Huang and Banghua Zhu and Jiaxin Huang},
      year={2025},
      eprint={2410.09724},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.09724}, 
}

@misc{agarwal2025persuasionoverridestruthmultiagent,
      title={When Persuasion Overrides Truth in Multi-Agent LLM Debates: Introducing a Confidence-Weighted Persuasion Override Rate (CW-POR)}, 
      author={Mahak Agarwal and Divyam Khanna},
      year={2025},
      eprint={2504.00374},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.00374}, 
}

@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@misc{wilie2024beliefrevisionadaptabilitylarge,
      title={Belief Revision: The Adaptability of Large Language Models Reasoning}, 
      author={Bryan Wilie and Samuel Cahyawijaya and Etsuko Ishii and Junxian He and Pascale Fung},
      year={2024},
      eprint={2406.19764},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.19764}, 
}

@misc{zhou2023navigatinggreyareaexpressions,
      title={Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models}, 
      author={Kaitlyn Zhou and Dan Jurafsky and Tatsunori Hashimoto},
      year={2023},
      eprint={2302.13439},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.13439}, 
}

@misc{liang2025introspectiveplanningaligningrobots,
      title={Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity}, 
      author={Kaiqu Liang and Zixu Zhang and Jaime Fernández Fisac},
      year={2025},
      eprint={2402.06529},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2402.06529}, 
}

@misc{hu2024uncertaintythoughtsuncertaintyawareplanning,
      title={Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models}, 
      author={Zhiyuan Hu and Chumin Liu and Xidong Feng and Yilun Zhao and See-Kiong Ng and Anh Tuan Luu and Junxian He and Pang Wei Koh and Bryan Hooi},
      year={2024},
      eprint={2402.03271},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.03271}, 
}

@misc{kobalczyk2025activetaskdisambiguationllms,
      title={Active Task Disambiguation with LLMs}, 
      author={Katarzyna Kobalczyk and Nicolas Astorga and Tennison Liu and Mihaela van der Schaar},
      year={2025},
      eprint={2502.04485},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.04485}, 
}

@misc{ren2023robotsaskhelpuncertainty,
      title={Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners}, 
      author={Allen Z. Ren and Anushri Dixit and Alexandra Bodrova and Sumeet Singh and Stephen Tu and Noah Brown and Peng Xu and Leila Takayama and Fei Xia and Jake Varley and Zhenjia Xu and Dorsa Sadigh and Andy Zeng and Anirudha Majumdar},
      year={2023},
      eprint={2307.01928},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2307.01928}, 
}
@misc{pawitan2024confidencereasoninglargelanguage,
      title={Confidence in the Reasoning of Large Language Models}, 
      author={Yudi Pawitan and Chris Holmes},
      year={2024},
      eprint={2412.15296},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.15296}, 
}
@inproceedings{groot-valdenegro-toro-2024-overconfidence,
    title = "Overconfidence is Key: Verbalized Uncertainty Evaluation in Large Language and Vision-Language Models",
    author = "Groot, Tobias  and
      Valdenegro - Toro, Matias",
    editor = "Ovalle, Anaelia  and
      Chang, Kai-Wei  and
      Cao, Yang Trista  and
      Mehrabi, Ninareh  and
      Zhao, Jieyu  and
      Galstyan, Aram  and
      Dhamala, Jwala  and
      Kumar, Anoop  and
      Gupta, Rahul",
    booktitle = "Proceedings of the 4th Workshop on Trustworthy Natural Language Processing (TrustNLP 2024)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.trustnlp-1.13/",
    doi = "10.18653/v1/2024.trustnlp-1.13",
    pages = "145--171",
    abstract = "Language and Vision-Language Models (LLMs/VLMs) have revolutionized the field of AI by their ability to generate human-like text and understand images, but ensuring their reliability is crucial. This paper aims to evaluate the ability of LLMs (GPT4, GPT-3.5, LLaMA2, and PaLM 2) and VLMs (GPT4V and Gemini Pro Vision) to estimate their verbalized uncertainty via prompting. We propose the new Japanese Uncertain Scenes (JUS) dataset, aimed at testing VLM capabilities via difficult queries and object counting, and the Net Calibration Error (NCE) to measure direction of miscalibration.Results show that both LLMs and VLMs have a high calibration error and are overconfident most of the time, indicating a poor capability for uncertainty estimation. Additionally we develop prompts for regression tasks, and we show that VLMs have poor calibration when producing mean/standard deviation and 95{\%} confidence intervals."
}